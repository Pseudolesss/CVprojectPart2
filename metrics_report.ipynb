{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Test in another folder that the zip send will work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Installation\n",
    "## Cytomine:\n",
    "Using pip, you can do it with these two lines:\n",
    "~~~\n",
    "$ curl -s https://packagecloud.io/install/repositories/cytomine-uliege/Cytomine-python-client/script.python.sh | bash\n",
    "$ pip install cytomine-python-client\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytomine import Cytomine\n",
    "from cytomine.models import ProjectCollection, ImageInstanceCollection, AnnotationCollection\n",
    "import getpass\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.affinity import affine_transform\n",
    "import cv2                             # OpenCV\n",
    "from imgTools import display, multiDisplay\n",
    "import os, sys\n",
    "import logging\n",
    "import segment_detector as sd\n",
    "import numpy as np\n",
    "from metrics_lines import metricNaive, metricFalseNeg, metricFalsePos, metricTot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image database\n",
    "The evaluation requires the database with all images. If you want to run the evaluation, please provide us the path to the folder(s) containing the images with the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to run the evaluation? (If not, what follows will not be needed) [y, n] : n\n"
     ]
    }
   ],
   "source": [
    "resp = input(\"Do you want to run the evaluation? (If not, what follows will not be needed) [y, n] : \")\n",
    "if resp == 'n':\n",
    "    EVALUATE = False\n",
    "    PATH_FOLDER_IMG = None\n",
    "else:\n",
    "    EVALUATE = True\n",
    "    print(\"Could you please provide us the path to the folder?\")\n",
    "    PATH_FOLDER_IMG = []\n",
    "    while True:\n",
    "        print(\"\\nThe current list of folders is : \")\n",
    "        for path in PATH_FOLDER_IMG:\n",
    "            print(f'\\t- {path}')\n",
    "        resp = input(\"Do you want to add a new folder? [y, n] : \")\n",
    "        if resp == 'n':\n",
    "            break\n",
    "        else:\n",
    "            resp = input(\"What is the new path? : \")\n",
    "            PATH_FOLDER_IMG.append(resp)\n",
    "\n",
    "if EVALUATE:\n",
    "    print(\"\\n\\nThe final list of folders is : \")\n",
    "    for path in PATH_FOLDER_IMG:\n",
    "        print(f'\\t- {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cytomine connection\n",
    "This is used to get the line annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public key of cytomine account : ········\n",
      "Private key of cytomine account : ········\n",
      "[2019-12-14 19:03:41,814][ERROR] [GET] [currentuser] CURRENT USER - 0 : None | 401 Unauthorized ()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "if EVALUATE:\n",
    "    host = 'https://learn.cytomine.be'\n",
    "    public_key = getpass.getpass('Public key of cytomine account : ')   # u can get it on your account\n",
    "    private_key = getpass.getpass('Private key of cytomine account : ') # u can get it on your account\n",
    "\n",
    "    CONN = Cytomine.connect(host, public_key, private_key, verbose=logging.ERROR)\n",
    "    # Check https://docs.python.org/3.6/library/logging.html#logging-levels  to see other verbose level\n",
    "    print(CONN.current_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of line detection\n",
    "## Metrics description\n",
    "To evaluate the line detection previously implemented, it was required to choose a metric. The metrics  for segment detection were designed from scratch as no satisfying metrics were found during our research.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**From annoted line parameters to annoted images**\n",
    "\n",
    "The images of the annoted line is obtained by drawing the line with a width of 3 pixels on a black image of the same size. The line width of 3 was chosen arbitrarily to take into consideration the degree of precision of the annotation which is not perfect.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Positive and negative classification**\n",
    "\n",
    "Let's define some important terms : \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "      \\text{line pixel } &= \\text{ pixel classified as a line by the annotation}\\\\\n",
    "      \\text{not a line pixel } &= \\text{ pixel classified as not a line by the annotation}\\\\\n",
    "      \\text{positive pixel } &= \\text{ pixel classified as a line by the dectection}\\\\\n",
    "      \\text{negative pixel } &= \\text{ pixel classified as not a line by the dectection}\\\\\n",
    "      \\text{true positive pixel } &= \\text{ line pixel classified as a line by the dectection}\\\\\n",
    "      \\text{false positive pixel } &= \\text{ not a line pixel classified as a line by the dectection}\\\\\n",
    "      \\text{true negative pixel } &= \\text{ not a line pixel classified as not a line by the dectection}\\\\\n",
    "      \\text{false negative pixel } &= \\text{ line pixel classified as not a line by the dectection}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Naive metric**\n",
    "\n",
    "The first implemented metric is :\n",
    "$$\\text{metric naive } = \\frac{p_c}{p_t} \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_c = \\text{ number of pixels well classified}\\\\\n",
    "      p_t = \\text{ total number of pixels}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "As the name indicates, it is a very naive metric as the number of line pixels are so low that an image with no lines detected will give a good result.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**False negative metric**\n",
    "\n",
    "The second implemented metric is based on penalization of the false negative pixels :\n",
    "$$\\text{metric false neg. } = min\\big(1, \\frac{1}{a_{neg}}\\frac{p_{tp}}{p_l}\\big) \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_{tp} = \\text{ number of true positive pixels }\\\\\n",
    "      p_{l} = \\text{ total number of line pixels}\\\\\n",
    "      a_{neg} = \\text{ acceptable accuracy for false negative}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "The acceptable accuracy is arbitrarly fixed at $a_{neg} = 0.8$. As the annotation are not perfectly made due to human error, a perfect accuracy should not be expected with this metric either. Thus, using this accepetable metric will simply give the best accuracy score for $\\frac{p_{tp}}{p_l}\\geq 0.8 $.\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "This metrics has the disadvantage of giving a perfect score for a method detecting each pixels as a line. Thus, a new metric is introduced : the false positive metric.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**False positive metric**\n",
    "\n",
    "This third metric is based on penalization of the false positive pixels:\n",
    "$$\\text{metric false pos. } = min\\big(1, \\frac{1}{a_{pos}}(max(0, 1-\\frac{3\\;p_{fp}}{p_{nl}}))\\big) \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_{fp} = \\text{ number of false positive pixels }\\\\\n",
    "      p_{nl} = \\text{ total number of not a line pixels}\\\\\n",
    "      a_{pos} = \\text{ acceptable accuracy for false positive}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "As $p_{nl}$ is usually a high number, this metric can easily give a high score, thus the acceptable accuracy for false positive is fixed to $a_{pos} = 1$ and the number of false positive is multiplied by 3 to have a higher penalization.\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "This metric has the opposite disadvantage of the false negative metric, a method detecting no line on the image will always give a perfect score. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Total metric**\n",
    "\n",
    "As the two previous metric had opposite disadvantage, it is easy to convince ourself that combining the two will obtain a new metric robust to those two cases.\n",
    "\n",
    "$$\\text{metric tot. } = w_1\\;\\text{metric false neg. } + w_2\\;\\text{metric false pos. }\\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      w_1 \\in [0,1]\\\\\n",
    "      w_2 \\in [0,1]\\\\\n",
    "      w_1 + w_2 = 1\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1]. The weight were both set to $w_1 = w_2 = 0.5$ such that both the method detecting lines on all pixels and the detecting no lines would have a score of about $0.5$.\n",
    "\n",
    "Note that the false positive metric is very simple and can easily give high score, thus changing it to a better one would give more representative results of the detection method accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example line metrics\n",
    "\n",
    "In this example, the metric for one image is given for 4 cases with the corresponding scores:\n",
    "\n",
    "$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- the actual detection } &: m_{tot} \\simeq 0.7\\\\\n",
    "      \\quad\\text{- all pixels as not a line } &: m_{tot} \\simeq 0.5\\\\\n",
    "      \\quad\\text{- all pixels as line} &: m_{tot} \\simeq 0.5\\\\\n",
    "      \\quad\\text{- all pixels randomly detected as line or not } &: m_{tot} \\simeq 0.3\\\\\n",
    "    \\end{array}\n",
    "\\right.$\n",
    "\n",
    "The score are as expected much better for the actual detection than the other ones. It can be noticed that the false positive metric is quite harsh. It gave a low score to the actual detection and a score of 0 to the randomly pixels. This will cause the total metric to only give good score for image with high number of true positive and very low number of false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cytomine with user : {conn.current_user}\n",
      "[2019-12-14 19:03:41,862][ERROR] [GET] [project collection] 0 objects | 401 Unauthorized ()\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-64cfb9c6bba3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics_line_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmetrics_line_example\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Master 2/Computer Vision/project/part 2/metrics_line_example.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mimgs_dict_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mprojects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProjectCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mproject\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprojects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mimage_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageInstanceCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_with_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_instances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "import metrics_line_example\n",
    "\n",
    "metrics_line_example.evaluate(CONN)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the database\n",
    "Using the metrics described above to evaluate our detection method on the full database will give the corresponding scores:\n",
    "\n",
    "\n",
    "$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric naive } &: m_{nai} = 0.945\\\\\n",
    "      \\quad\\text{- metric false neg. } &: m_{neg} = 0.725\\\\\n",
    "      \\quad\\text{- metric false pos. } &: m_{pos} = 0.860\\\\\n",
    "      \\quad\\text{- metric total } &: m_{tot} = 0.793\\\\\n",
    "    \\end{array}\n",
    "\\right.$\n",
    "\n",
    "The false negative metric has a good accuracy, however, it still has room for improvement. The false positive metric is very high which will improve the total metric accuracy. As explained earlier, the false positive metric is simple and easily give high score, even though it is not that good to the human eye. However it was difficult to find another metric to penalize correctly the false positive pixels.\n",
    "\n",
    "In general, the evaluation indicates that our detection method has good accuracy but could have been better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluateSegDetec\n",
    "\n",
    "if EVALUATE:\n",
    "    resp = input(\"Do you want to evaluate the line detector of part1? \\nThis will takes 2-3 minutes for the full database. [y, n] : \")\n",
    "    if resp == 'y':\n",
    "        evaluateSegDetec.evaluate(PATH_FOLDER_IMG, CONN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of ellipse detection\n",
    "## Metrics description\n",
    "### Distance between two ellipses\n",
    "It is assumed that the ellipses are defined by 5 parameters:\n",
    "- Xc : the vert. coord of the center of the ellipse\n",
    "- Yc : the horiz. coord of the center of the ellipse\n",
    "- theta : the angle of the main axis\n",
    "- a  : half the length of main axis\n",
    "- b  : half the length of sub axis\n",
    "\n",
    "~~~\n",
    "elps1 = [Xc1, Yc1, theta1, a1, b1]\n",
    "elps2 = [Xc2, Yc2, theta2, a2, b2]\n",
    "W = [Wx, Wy, Wt, Wa, Wb]\n",
    "dist = Wx * abs(Xc1-Xc2) + Wy * abs(Yc1-Yc2) + Wt * min(abs(theta1-theta2), abs(180-abs(theta1-theta2))  + Wa * abs(a1-a2) + Wb * abs(b1-b2)\n",
    "~~~\n",
    "The weight W is tuned such that the maximal acceptable error on each parameters will lead to an increase in the distance of 1. Thus if we want an acceptable error of theta of 15 degrees, we fix Wt = 1/15. This way, we can fix the tresh_dist = 5 (see next subsection). This is the maximal acceptable distance between two ellipses. By fixing it at 5, we only need to tune W. The parameters W will depend on the precision expected on the labelled ellipse. \n",
    "\n",
    "Note that as a difference of 180 degree in theta correspond to the same angle, 'min(abs(theta1-theta2), abs(180-abs(theta1-theta2))' is used instead of 'abs(theta1-theta2)'.\n",
    "\n",
    "In practice, the weights used are : $$W = [\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{5}, \\frac{1}{3}, \\frac{1}{3}]$$\n",
    "\n",
    "Example : If we assume that the label for the x coord. of the center is has a precision of two pixels in, i.e there is a two pixels difference between the true x coord and the x coord. labelled, then we can put Wx = 1/2.\n",
    "\n",
    "### Metric on multiple images\n",
    "Let ground_truth and detected both being a list of list of ellipses, respectivelly being the labeled ellipses for each images and the detected ellipses for each images. The evaluation is done on the following pseudo-code:\n",
    "\n",
    "~~~\n",
    "metrics_elps(ground_truth, detected, tresh_dist):\n",
    "    eval = 0\n",
    "    for each image:\n",
    "        gt_elps = ground_truth of the corresponding image\n",
    "        dt_elps = detected of the corresponding image\n",
    "\n",
    "        num = 0\n",
    "        denom = max(len(gt_elps), len(dt_elps))\n",
    "\n",
    "        dists = np.zeros((len(gt_elps), len(dt_elps)))\n",
    "        for el in dists:\n",
    "            el = dist(gt_elps[index1(el)], dt_elps[index2(el)]\n",
    "\n",
    "        index1_rem = []\n",
    "        index2_rem = []\n",
    "        for i in range(min(len(gt_elps), len(dt_elps))):\n",
    "            el = min(dists) where (not index1(el) in index1_rem) and (not index2(el) in index2_rem)\n",
    "            index1_rem.append(index1(el))\n",
    "            index2_rem.append(index2(el))\n",
    "\n",
    "            num += min(1, tresh_dist/el)\n",
    "\n",
    "        eval += num/denom\n",
    "    return eval/num_image\n",
    "~~~\n",
    "\n",
    "In other words, the evaluation metric give a value between 0 and 1, 1 being the best accuracy. The evaluation for multiple images is the mean of all image evaluations. For each image, the evaluation of this images is a fraction. Let $$f(x) = min(1, \\frac{tresh\\_dist}{x}),$$ a function which returns 1 if x < tresh_dist and something < 1 in other cases. The choice of the tresh_dist is done such that a distance lower than it doesn't make sense compared to the precision of the labelling.\n",
    "\n",
    "The numerator is the sum of function f on the smallest distances between the detected ellipses and the labeled ellipses where each detected and labeled ellipse can only be used once. When the number of detected and labeled ellipses aren't the same, the distance are computed until no pair can be made.\n",
    "\n",
    "In the other hand, the denominator is the maximum between the number of detected and labeled ellipses. Thus the value can decrease due to two reasons : if the detection detect less or more ellipses than in the ground truth and if the detected ellipses are distant from the labeled ones.\n",
    "\n",
    "\n",
    "In the special case where there is 0 ellipse in the ground truth the score is 0 if ellipse are detected or 1 if no ellipse are detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of evaluation\n",
    "In the example bellow, the metric for different detection will be displayed. As explained above, the number of ellipse detected will influence the final metric value. To better understand the detection strength and weakness, the metric with or without the penalization for detecting more/less ellipse than existing ellipse will be used.\n",
    "\n",
    "It is tested on 5 cases:\n",
    "    - Same parameters with a 180 change in theta\n",
    "        => metric = 1 for both\n",
    "    - Two ellipses detected with wrong parameters \n",
    "        => it is seen that detecting x times more ellipse than existing ones will divide the score by x (x = 2 here).\n",
    "    - Detecting an ellipse with parameters in the limit of the acceptable error\n",
    "        => metric = 1 for both.\n",
    "    - Detected ellipse close to the ground truth but not exactly it.\n",
    "    - The ground truth detected but other too \n",
    "        => it will give a low score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (metrics.py, line 28)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/roekens/anaconda3/envs/envComputerVision/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-2-02e401d95e43>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import metrics_elps_example\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/roekens/Documents/Master 2/Computer Vision/project/part 2/metrics_elps_example.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from metrics import metric_elps\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/roekens/Documents/Master 2/Computer Vision/project/part 2/metrics.py\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    diff2 = k.abs(k.abs(elps1 - elps2)- np.array([0, 0, 180, 0, 0], dtype=np.float32))) # array OK for keras?\u001b[0m\n\u001b[0m                                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import metrics_elps_example\n",
    "\n",
    "metrics_elps_example.evaluate()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the classical method on the database\n",
    "Using the metrics described above to evaluate our detection method on the full database will give the corresponding scores:\n",
    "\n",
    "- On the eye images with 1 ellipse\n",
    "$$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric penalizing } &: m_{p} = 0.36\\\\\n",
    "      \\quad\\text{- metric non penalizing } &: m_{np} = 0.58\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "- On the eye images with 0 ellipse\n",
    "$$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric penalizing } &: m_{p} = 0.21\\\\\n",
    "      \\quad\\text{- metric non penalizing } &: m_{np} = 0.21\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "In the case of the images with 1 ellipse, the average number of ellipses detected is : $\\frac{0.58}{0.36} = 1.61$ which is pretty good. However, in the case of 0 ellipse on the image, only 1 image out of 5 did the algorithm detect no ellipse which isn't that good. Going back to the case of 1 ellipse, the metric non penalizing reach a good accuracy.\n",
    "\n",
    "Giving our weights, assuming that the error in split equally on the parameters, the ellipse detected will have a distance to the ground truth ellipse of : \n",
    "$$\\frac{2}{0.6} = 3.33\\text{ pixels of Xc}, 3.33\\text{ pixels of Yc}, 8.33\\text{ degrees of theta},5 \\text{ pixels of a},5 \\text{ pixels of b}  $$\n",
    "\n",
    "In conclusion, the classical method has a good accuracy but struggle to detect the exact number of ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to evaluate the line detector of part1? \n",
      "This will takes several minutes for the full database. [y, n] : y\n",
      "\n",
      "The folders containing the images are \"\n",
      "['images_database/eyes/noEllipses/full/', 'images_database/eyes/preprocess/']\n",
      "\". Do you want to change them? [y, n] : n\n",
      "Progress : 0.16% \t - Value m1 = 0.35 \t m2 = 0.70    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0aa5e70a83ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do you want to evaluate the line detector of part1? \\nThis will takes several minutes for the full database. [y, n] : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mevaluateElpsClassical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_FOLDER_IMG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Master 2/Computer Vision/project/part 2/evaluateElpsClassical.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(folders)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCanny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdetect_elps_classical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mm1i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_elps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mann\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master 2/Computer Vision/project/part 2/evaluateElpsClassical.py\u001b[0m in \u001b[0;36mdetect_elps_classical\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Detect ellipses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0melps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myoughongQiangEllipse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageBinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master 2/Computer Vision/project/part 2/classical.py\u001b[0m in \u001b[0;36myoughongQiangEllipse\u001b[0;34m(image, minDist, maxDist, minLength, maxLength, bags, threshold, ratio)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpixels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdSq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminDist\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfSq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master 2/Computer Vision/project/part 2/classical.py\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(pt1, pt2)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0msqdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import evaluateElpsClassical\n",
    "\n",
    "if EVALUATE:\n",
    "    TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOo = > preprocess\n",
    "    resp = input(\"Do you want to evaluate the line detector of part1? \\nThis will takes several minutes for the full database. [y, n] : \")\n",
    "    if resp == 'y':\n",
    "        evaluateElpsClassical.main(PATH_FOLDER_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of bounding box detection\n",
    "## Metrics description\n",
    "### Distance between two ebounding box\n",
    "The metric and distance are the same as for the ellipse. The only differences is that there are only 4 parameters (the coordinates of the bottom left corner and top right corner), the weight are $W = [\\frac{1}{2},\\frac{1}{2},\\frac{1}{2},\\frac{1}{2}]$ and the tresh_dist is now equal to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of evaluation\n",
    "In the example bellow, the metric for different detection will be displayed. As explained above, the number of ellipse detected will influence the final metric value. To better understand the detection strength and weakness, the metric with or without the penalization for detecting more/less ellipse than existing ellipse will be used.\n",
    "\n",
    "It is tested on 5 cases:\n",
    "    - Same parameters\n",
    "        => metric = 1 for both\n",
    "    - Two boxes detected with wrong parameters \n",
    "        => it is seen that detecting x times more box than existing ones will divide the score by x (x = 2 here).\n",
    "    - Detecting a box with parameters in the limit of the acceptable error\n",
    "        => metric = 1 for both.\n",
    "    - Detected box far from the ground truth.\n",
    "    - The ground truth detected but other too \n",
    "        => it will give a low score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start evaluation...\n",
      "\n",
      "\n",
      "The ground truth ellipse is : \n",
      "\tXldc = 50.0, Yldc = 100.0, Xruc = 150.0, Yruc = 50.0.\n",
      "\n",
      "--------------------------------\n",
      "The metrics for the ellipse(s) :\n",
      "\tXldc = 50.0, Yldc = 100.0, Xruc = 150.0, Yruc = 50.0.\n",
      "Penalizing metric =     1.0\n",
      "Non penalizing metric = 1.0\n",
      "\n",
      "--------------------------------\n",
      "The metrics for the ellipse(s) :\n",
      "\tXldc = 60.0, Yldc = 80.0, Xruc = 170.0, Yruc = 30.0.\n",
      "\tXldc = 60.0, Yldc = 120.0, Xruc = 150.0, Yruc = 70.0.\n",
      "Penalizing metric =     0.1600002719997824\n",
      "Non penalizing metric = 0.3200005439995648\n",
      "\n",
      "--------------------------------\n",
      "The metrics for the ellipse(s) :\n",
      "\tXldc = 52.0, Yldc = 98.0, Xruc = 148.0, Yruc = 52.0.\n",
      "Penalizing metric =     1.0\n",
      "Non penalizing metric = 1.0\n",
      "\n",
      "--------------------------------\n",
      "The metrics for the ellipse(s) :\n",
      "\tXldc = 60.0, Yldc = 130.0, Xruc = 80.0, Yruc = 70.0.\n",
      "Penalizing metric =     0.12307719289932524\n",
      "Non penalizing metric = 0.12307719289932524\n",
      "\n",
      "--------------------------------\n",
      "The metrics for the ellipse(s) :\n",
      "\tXldc = 60.0, Yldc = 130.0, Xruc = 80.0, Yruc = 70.0.\n",
      "\tXldc = 50.0, Yldc = 100.0, Xruc = 150.0, Yruc = 50.0.\n",
      "\tXldc = 60.0, Yldc = 120.0, Xruc = 150.0, Yruc = 70.0.\n",
      "Penalizing metric =     0.3333333333333333\n",
      "Non penalizing metric = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAANPCAYAAADZhU44AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdefwsd13n+/cnnJAVCEmAS4IkNyD7gF4dNlmCRJERRAx4NUhgRlSGcbsoqDAadxEUBxGSYR7sq1wQAYcwohCGbQZRJNyMIYyQsBkgZCGbivC9f1T9kj6d3++c38kn5/y25/Px6EdOd3VXV3V31a9eXVWdGmMEAACAG++gjZ4AAACArU5YAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsGKvqurCqjrlAD/nr1bVaw7kcwKbQ1VdVVUnbfR0AOtnuQVhtSlU1Q9V1f+sqqur6kvzv59WVbXR07Y3VfWKqvrN5jhOrqrP3VTTBGxOVXVOVT1lb/cbYxw5xvjUAZqm21fV26rqC1U1qurEvdz/xKp6T1VdU1XnH+gvneBA24zLbZJU1WlVddG87fSnVXX0Hu77kqr6RFV9o6qevMrwk6rqz6rqyqq6pKqeuzDs6Kp6y/w8F1XVaeudjhv72Ko6pKpeOg+7sqo+WlWPXHjcE+aQXblcM6+/vm0e/rB5PXVFVV24yvw+sKo+PI/73Kp60MKwqqpnV9VnquqrVfWGqrrlwvDjq+qtVXVpVX2uqp66MOzBS9N11Txdpy6M+zer6vPztJ1TVfdcePzvVdUn5+k6v6pOX+P9fNI83r1+Lg80YbXBqurnkrwgyfOS/B9JbpfkqUm+I8nN13jMzQ7YBDZV1a6NngZga9ig9cU3krwzyanrvP/rk3w0yTFJnp3kTVV1m/00bbDpbcRyO2+M/+ckT8y03XRNkhfv4SEfS/K0JH+zyrhunuRdSd6daTvsDkkWj5h5UZJ/np/nCUnOXImBdUzHjX3sriSfTfLQJLdK8stJ3rjyxc8Y47VzyB45xjhynrdPLczf1UleluQZq8zv0Unelmm786gkz03y9qq69XyX0+dp+o4kxyU5LMkLF0bxmiSfnqf5e5P8dlU9bJ6u9y1N16OSXJVpHZskj0/y75I8OMnRST6U5NUL4746yaPneX5SkhdU1QOXpv/WSX4pyXnL87YpjDFcNuiS6YNzdZJT93K/VyQ5M8k75vufMj/2VUm+nOSiJP8xyUHz/X81yWsWHn9ikpFk13z9nCS/keQDSa5M8udJjl24/xPncX4l04bDhUlOWWW6fjzJ1zKtNK5K8vb59guT/EKSc5P8U6YVxEhy56V5+s0kRyS5NtPGzVXz5bh5Ht44z+OVmRagb9/o98zFZadd5uX5GfPyfHWSl2b6g3r2vGz+RZJbL9z//kk+mOTyTBszJ8+3/1aSryf5x3k5/6P59pHkPyT5ZJJPL9x25/nfhyX5/XmddEWS9yc5bD/M58p66sQ93Ocu8zrtFgu3vS/JUzf6fXJxWbxs9+U2yW8ned3C9TvN2yK32Mvj3p/kyUu3/XiS961x/yPm8d5l4bZXJ3nO3qaj89g1puXcrLG9mOQ9Sc5Y5fZTkly4dNujkpy3dNsFSX50/vebkjxjYdgD5/f/8CRHzu/zbRaGvyTJq9eYrpcnefnC9V9I8saF6/dM8o97eL/eluTnlm47K1NInpPkKRu5nK12scdqYz0gySFJ3rqO+56WaQV3i0wrhhdmiquTMn2jcXqSf7sPz33afP/bZtoz9vNJUlX3yBRxT8wUOMdk+vbmBsYYL0ny2iTPHdO3E49eGPzDmb7JOGqM8S9rTcQY4+okj0zyhXH9txxfmAd/X5I3ZPpG5W1J/mgf5g+46Zya5LsyhcWjM22cPSvJsZmOfPjpZDpEJMl/zfSlydGZ1itvrqrbjDGenSlCfnJezn9yYfzfn+R+Se6xynP/XpJvy/TH/egkz8z0RcxuquqOVXX5Hi6nLT/mRrhnkk+NMa5cuO1j8+2w2Wzn5faemZa9JMkY4+8zR8x6Xpgl909yYVWdXdNhgOdU1b+ah90lydfHGBcs3H9xmd/TdHQeu5uqut18+w320lTVCUkekumL6PWo+bJ8273WGF6ZtlW/eeH25eH3ypKqOjzJ45K8cuHmNyS5c1XdpaoOzrRX6p3Lj50ff1iSf52Fea6q+yb59kxxtSk5TGtjHZvkksXwqKoPZlpJHZLkEWOM/z4PeusY4wPzfb6W5P9O8q3zH/grq+r3M8XQS9f53C9fWdir6o2ZIiaZFoI/W3neqvrlJD+5+ij26A/HGJ+9EY9b9P4xxjvm6Xh1kp9tjg+4cV44xvhiklTV+5J8aYzx0fn6W5I8fL7fjyR5x8pym+RdVfWRJP8mu/9xXfY7Y4xLl2+sqoMyHTZy/zHG5+ebP7jaCMYYn8n0Jcz+dGSmb98XXZHk+P38vHBjbOfldq1l8RY3Ylx3SPKwTNtBf5nkZ5K8taruto7n2dPwrzcee505QF6b5JVjjPNXmf7TM+1x+/Qa87fsg0mOq6ofzrR36rRMe8sOn4efneSZ87bhZZn2MiXJ4WOMK6vqA0l+uaqekWl79dRMR08tOzXJJUneu3DbP2QK9U9ken0+m+Q715jOszKF539LrjsN5sVJfmqM8Y3apD9DYI/VxvpKkmMXj08eYzxwjHHUPGzx/VmMlGMz7WW6aOG2i7Jvf9wvXvj3NZkW8GTaS3Xdc817lL6yD+Nd0Y2q5IbTeKhztmBDfHHh39eucn1l/XFCkscvfuOc5EFJbr+X8a+1vjg2yaFJ/n7fJ3m/uCrJLZduu2WmQ6tgs9nOy+1NuSxem+mL3LPHGP+caW/bMUnuvo7n2dPwzmOTXBepr860J2utL7lPz54DeDdjjK8keUySp2f6THxPpkNDV35E7GWZziU9J9PeovfMt68Mf0KS/zPT+39mpuhb7QfInpTkVWM+fm92Rqa9UN+U6TPya0nePe/duk5VPS/TXrAfXHj805KcO8b40HrndSMIq431oUzH6z9mHfdd/GBekuncphMWbrtjkpVvhq7O9d88JNPJmOv1D5k+8Emu25V7zDqna0+3X7OHaVprHMDW8tlMx9oftXA5YozxnHn4etcXKy7JdGz/nfb2xPMhRcu/RrV4ecK+z84NnJfkpKpa/Eb5PtmsJ1HD+mzF5fa8TMveynhOynSkzwVr3H9Pzs3a83JBkl1V9c0Lty0u83uajs5jU9MumZVz404dY3xteeKqauUHJt60nhldMcZ47xjjX48xjs50tNNdk3x4HvaNMcYZY4wTxxh3mKfz8/MlY4yLxhiPGmPcZoxxv0zbiB9emq5vSnJybnh44n2S/PEY43NjjH8ZY7wiya2zcDhpVf1aplNEvnuM8dWFxz48yWOr6uKqujjTYaa/X1Wb6jQRYbWBxhiXZ6r1F1fV46rqyKo6qKq+JdNJj2s97uuZftjht6rqFvPxtU/P9b9i87dJHjKvsG6V6ddT1utNSR5VVQ+afynn17Pnz8kXM53ntTd/m+S0qrpZVX1PpvPCFsdxzDytwNb1miSPrqpHzMv6oTX97xRWztNc7/oiyfQHPtO3p8+vquPmcT6gqg5Z5b6fWThPc7XLa9d6nqo6NNMGTZIcMl9fbXouyLQuO2Oet8cmuXeSN693nmAT2orL7WvnaX5wVR2RaVvlT5bOf7xOVd18Xq4rycHzPK5s27wmyf2r6pT5cLOfzRSHfzcftfMnSX69qo6YQ+Yxuf6X7Nacjs5j5+FnZtpr9ugxxrVrvA5PSvLm5fmetyUPTXLwdLUOnbfpVoZ/a1UdXNPPqP9eks+NMVYOuTu6qu5Uk3skeX6SX5/f11TV3edtz5tX1Y8k+e75PouemOSD83lji/4q097R283T+MR5Gv/3PO5fynRo4nfNe9YWPXl+Pb5lvnwk0zb0s9d4bTaEsNpgY4znZoqiZyb5UqYV2H/OdEzrqsckz34q056pT2X6MYvXZVqRZYzxriR/nOlbmL9O8mf7MD3nZfqln9dl2nt1WVbfxbvipUnuMR8+8Kd7uN/PZDp59vJMu5Gvu+98zPDrk3xqHs9x651eYPOYz6t8TKYT5L+c6ZvwZ+T6vzUvSPK4qrqsqv5wnaP9+SQfz/QH+dIkv5ub/m/XtZkOy0mS8+frSZKqOquqFk+U/qFMJ09fluQ5SR43xljt/ALYErbicjtvqzw1U5x8KdN5SU9bGV7TD1E8a+Ehf55puX5gpl+xuzbTDz5kjPGJTOeZnZVpuX5Mku+bDwvMPN7D5ud5fZJ/Pz//Xqfjxj52/sL8JzIFxMWr7cGbw+kHs/phgA+Z5/EdmY5ounZ+DVY8M1M8fjbTIZ+PXRh2bK7/Feqzk7xsTD9WtuIRmbY9L5un/3tWWQeudXji72Y6b+pvM20P/j+Z9sZdPg//7Xl6P7kwz8+aX6/LxxgXr1wyHR751THG8nlqG6p2P/QRAACAfWWPFQAAQJOwAgAAaBJWAAAATcIKAACgaZ/+Z6tV5ZcuYBsaY2zO/4X5Olk3wfa01ddNifUTbFerrZ/ssQIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoGnXRk8AAAA729joCWDTq42egHWwxwoAgA0jqliPrfA5EVYAAABNDgUEAGDT2AqHfHHgbIU9VSvssQIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANO3a6AnYCsYYGz0JHEBVtdGTAOtizbSzWDOxlezTttPS313bXVuPbaeJPVYAAABNwmovfGuy84wxvO9sej6hO8+I952twd/Qnce200RYAQAANDnHah84fnR7800LW5U10/ZmzcRWtp5tp+XPuO2trcO20+7ssQIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANO3a6AkAAIAVY6MngPWrmv47vGuJsAIAADpWAmuHcyggAABAk7ACAABociggAACbhoPKtg5nVu3OHisAAIAmYQUAANAkrAAAAJqEFQAAQJOwAgAAaBJWAAAATcIKAACgyf/HCgCATcP/G2nr2unvnT1WAAAATcIKAACgSVgBAACbWm30BKyDc6wAANg0tsIGNJPlc6p2+ntnjxUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoGnXRk/AVjLG2OhJALgBayZgs1rXtlPVvj+GzWHpvdvp7LECAABoElYAAABNwmovyi7OHaeqvO9sej6hO0/F+87W4G8oO5VzrNbBCgLYjKyZgM1qX7adls+ost21dTgbbnf2WAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmnZt9AQAAMCKsdETADeSPVYAAABNwgoAgA1TGz0BtFW8j4lDAQEA2GA2ytkO7LECAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBp1z7e/5IkF+2PCQE2zAkbPQE3Aesm2H62w7opsX6C7WjV9VONMQ70hAAAAGwrDgUEAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwmqHqqqrquqkjZ4OYM8sq8BWYF0Fwmrbqapzquope7vfGOPIMcanDtA03b6q3lZVX6iqUVUn7uX+J1bVe6rqmqo6v6pOORDTCQfSZlxWk6SqTquqi6rq6qr606o6eg/3fUlVfaKqvlFVT14a9qSq+uuq+mpVfa6qnltVuxaGv6aq/mEefsHya1FVD5+X/2vm9cEJC8POmzfiVi7/UlVvn4c9eGnYVfN659R5+L2q6r9V1SVVNVaZp7tX1bur6oqq+t9V9diFYfevqndV1aVV9eWq+n+r6vYLww+pqrOq6ovzfd5eVcevZ9zz8KfMt19VVe+squMWhv1sVX1qfr2+UFV/sPJ6VtVtq+r18+1XVNUHqup+C489eX6PFl+TJy1M80vn9/zKqvpoVT1yjff7jPm1tE7eQbb6uqqqjp2Xia9U1eVV9aGq+o6F4T80r8euqKovVdUrq+qWC8PXXFdV1T2q6iNVddl8+YuqusfC8DWX23n4e+Z1yVer6mNV9ZiFYQ+rqo/P0/yVqnrL0vrkuVX12fmxF1XVs3fyPG8aYwyXbXRJck6Sp+xh+K4NmKbbJXlakgckGUlO3Mv9P5Tk+UkOS3JqksuT3GajX1sXl5vyskmX1XsmuTLJQ5IcmeR1Sd6wh/v/hyQPT/KRJE9eGvbvkzw4yc2THJ/kr5P84tJzHTL/+25JLk7ybfP1Y5NckeTxSQ5N8rwk/2ONaagkn0py+hrDT57n6Yj5+l2T/GiSx0x/And/zZNckOTpSW6W5DuTXJ3kLvPwR87TdMskhyd5WZJ3Ljz+mUk+Nq/zDk3y6iR/ss5xPzTJl+bX5eZJzkzy3oVx3ynJUfO/j07y7iRPn6+fNI/39vO4fzzJJUmOXHgNPrfG63NEkl9NcmKmL1sfNb9eJy7d705JPp7kC0lO2ejlx+XAXbb6umpeFu86f74ryfcnuXRlupN8U5Jj538fmeS1Sf5w6bnWWlcdNS87NS97P53k3IXHrrnczrfde2E67jfP0+3n67dLctz870OSPDfJ2xYee9dcv147Psl5SX5gp87zZrls+AS4jCS5MMkzkpyb6Q/tS+cP19nzB+4vktx64f73T/LBTMHxsSQnz7f/VpKvJ/nHJFcl+aP59pFpA+iTST69cNud538fluT3k1yUaWPm/UkO2w/zuSt7Caskd0nyT0lusXDb+5I8daPfJxeX7b6sJvntJK9buH6nJP+8uDyu8bj3ZymsVrnP05O8fY1hd03yD0l+cL7+40k+uDD8iCTXJrnbKo996PwaHrHGuF+e5OWr3H7n3DCs7jWPqxZu+/Mkv7HGuP+vJFcuXD8zyXMXrn9vkk+sZ9xJfi/JixaGHTe/93da5XmPmT9rL97D6/3VXL8hdHLWCKs1HntuklOXbjs7yb+ZlwFhtckv1lVrPu6gJI+ep/W2qww/Msmrkrxjjcfvtq5aGrZrfk2uWeOxe1xuk9x3fp3vu8qwQ5L8TpL/tcZjj8/0xcczzfMGL3sbPQEu160A/8e80js+07eWf5PkW+cP1ruTnDHf9/gkX8n0B+6gJN81X7/NPPycLH2zNC9M78r0zcFhC7etrABfND/u+EzfPjww8zcVS+O5Y6aV7lqX0/Yyn+sJq8cm+bul2/4oyQs3+n1ycdnuy2qStyb5haXbrsq8gb6H12U9YfWnSZ6zdNuLk1wzz+Pf5Po9LC9IcubSff+/LG3sz7e/LMkr1njOwzNtRJ68yrDVwupf5Ybx864kb1lj/D+bhT1pSb49yQcyRdHhmb5F/0/rGXemjdAXLww7fn5dHrNw22mZgmkk+XKS+6wxXd+SaWPlVvP1kzNtdH4xyaeT/EHWDtHbzY+928Jtj0/y1oVlQFht8kusq1Z7zLnzcjCS/JelYQ/KFIAjU4h+99LwVddVC8MvT/IvSb6R5D8uDdvjcpvkz+ZlbiR5Z5KDVnl9vpHka7nhkQG/OM/3yLTn/g47fZ43+nLdMY9suBeOMb6YJFX1viRfGmN8dL7+lkyH2yTJj2T6VuEd8/V3VdVHMq0QX7mH8f/OGOPS5Rur6qAk/y7J/ccYn59v/uBqIxhjfCbTLuD96chMC/qiKzKtnGEz2M7L6lrL3y1uxLiuU1X/NlN07HaexhjjaVX1U5kOEz45097qlen48t6mo6oOT/K4JN+3xlOfmumQuPeuc1LPz7QB+oyq+oMkD8u0R+w9q8zTvZP8SqZDCldckOQzST6f6Vv+jyf5yXWO+x1J/riqzsq0F+BXMm04HL4y8jHG65K8rqq+OcnpmUJpebpumekQxF8bY6y8l+dniq3zk5yQ6fP3/CQ/sfTYgzMdEvTKMcb5821HZto78N1rvGZsXtZVuz/Xvavq0Exf4N58adj7k9xqPp/nxzKF6eLwtdZVK8OPqqojkjwp0166xWF7XG7HGI+al71TMn2h8Y2FYZ9JctR8/tiPZVqGFx/7nKr63UzL9/cvvyY7cZ43mh+v2DwWP3TXrnL9yPnfJyR5/Hxi3+VVdXmmbx1unz377Bq3H5vpWNy/3/dJ3i+uynT+wqJbZvrWGTaD7bys3uTLX1V9f5LnJHnkGOOS5eFjjK/Pf+DvkOm8rH2Zjh/IdN7AWuH0pCSvGvPXnHszxvhapj/U35vpnIKfS/LGJJ9bmqc7Zzqk6mfGGO9bGHRmpvfomEyHL/7JfL+9jnuM8ZdJzkjy5kwbKRfO87vbc8/3/WSmcwtevDRdhyV5e6a9aL+zcP+Lxxj/a4zxjTHGpzOdC/a4pccelCnI/jnXx2CS/FqSV8+PY2uxrloyxvjHMcbrk/xiVd1nleGfz7QH5Q2rDFttXbU4/OokZyV5VVXddpXhqy6387CvjTHOTvKIqrrBF0VzwL4yyVsXfwhiHjbmYL420/K64+d5I4piP2wAAA5jSURBVAmrreezmf7IHbVwOWKM8Zx5+FobEGvdfkmm3bF32tsTV9Ud64a/trV4ecK+z84NnJfkpKpa/NbpPvPtsJVsxWX1vEzL28p4Tsp02NAFe3vONabje5L8lySPHmN8fC9335Xr5215Oo6Yhy2vB9YMp6r6pkzfsr5qX6Z5jHHuGOOhY4xjxhiPyPTDEB9eGO8Jmc4Z+I0xxquXHn6fTIclXjrG+KckL0xy36o6dj3jHmO8aIzxzWOM22YKrF2ZDoFczeLrlao6JNPhlp/P0p6o1WYz04nnK4+tXH8OzqlzBK54eJKfrqqLq+riTCe9v7GqfmEvz8HWsRPXVQdnWv5Ws9uytY/DD8q0l3mto2w6496V5La5YVCud9w7cZ4POGG19bwmyaOr6hFVdbOqOrSmn9K9wzz8i1l7wbmBeffry5I8v6qOm8f5gPmP9PJ9PzOmn1Nd6/LatZ5n3hW9Ms5D5uurTc8FSf42yRnzvD020y/IvHm98wSbxFZcVl87T/OD55j59Uy/arfqt8BVdfN5Wa4kB8/zeNA87Dvn8Z06xvjw0uNuW9PP/R45z8cjkvxwpvM+kuQtSe5VVafO4/+VTL86df7COO6Q6XC6tQ5VemKmH8DY7Vvzmhya+bCYeZoPWRh+7/m2w6vq5zN9a/+Kedjx8zS+aIxx1irP+VdJTq+qW82HuTwtyRdW9tTtZdyH1vRT8FVVd0zykiQvGGNcNg9/yso3wjX9tPEvJfnL+frBSd6U6dvb0xcPq5mHnzxvwNYcnM/JdI7KijOT3D1TAF+7NE8Pz/TDG98yX76QKdxetOqrzla0rddVNf1vEh40r68Om78UuF2S/zkPf8LC8nFCph/sWFm29riuqqrvqqpvnYfdMtMhtpcl+bt5+J6W27tV1SPnaTq4qn4k068cvnce/gNVddeqOqiqbjOP+6NjjEvn236iqm49T/d9M/2IxMq4d9w8bxpjE5zotdMvWToZONNK7lcXrj8lyV8sXL9fpg/hpZnOQ/ivSe44D3tApm9sLsv805lZOKF0YRyLJ5keluQ/Zfqm84ok/z038a8Czs+322Vh2FlJzlq4fmKmk16vTfKJOFHaZZNcdsiyelqm84SuzrTxffTCsLOTPGvh+jmrLNsnz8Pek+nE5qsWLmfPw24zvy6XZzrB+eNJfmxpOk7JdGz9tfPznLg0/JeSvG8P83F+kh9d5fYTV5nmCxeGP29+T66a5/fOC8POmO+/OE9XLQw/JtMG35fmeXt/Fn7tai/jPirX/4LbxZl+DetmC8NfnmkD9+r5c/i8JIfOwx46T9c1S9P24Hn40+fPzDWZ9k68MPOvp2U6DGzk+l99W7k8YT3LgMvmvCy/T9nB66p5+fhYpsMEVw4dfsjCfX8r0yG3V8//fUmSY+Zhe1xXZfphl/PnZebLmc6VvPfC8D0tt3fPFDpXzuP/qySPXXjsT2X6sZmVdcIbkpwwDzso0+F7l87PfUGSZ2X+cZydOM+b5bLyBgAAAHAjORQQAACgSVgBAAA0CSsAAIAmYQUAANC0a+93uV5V+aUL2IbGGLX3e21e1k2wPW31dVNi/QTb1WrrJ3usAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABAk7ACAABoElYAAABNwgoAAKBJWAEAADQJKwAAgCZhBQAA0CSsAAAAmoQVAABA066NnoCtYIyx0ZPAAVRVGz0JsC7WTTuLdRNbibXTzmLtNLHHCgAAoElY7YVvhHeeMYb3nU3PZ3TnsW5iq/Ap3XlGvO+JQwH3jcMwtjcbLAAA3EjCar1E1fZXJa7Ykpx7s73ZS8VWZu20vVk77c6hgAAAAE3CCgAAoMmhgDeSXdvbh93YAAB02WMFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQtGujJwCAA2McoOepA/Q8wPZxoNZPsD/ZYwWwAxzIjRYbSMC+sM5guxBWAAAATcIKAACgyTlWADvQ/jgPyuE8wE1hf6+fnAd607He3509VgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACadm30BGxVY6MnAAAA2DTssQIAAGiyx2q9xkiqNnoq2J+G/ZBsTWM9n92l9de6HrOvFp5jv4wf2HJuzJpgf689rJ3YX4TVvrChAAAArMKhgAAAAE3Cai/K4X87TlV539n0fEZ3HusmtgqfUnYqhwKugz9kwGa0L+um5QOZ98d6bfE5rDdhZ9uXNcAN1k835YSs8hzWTjcdJ8nszh4rAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAEDTro2eAAAAWDG2+PjZueyxAgAAaLLHCgAAuNFqoydgkxBWAABsGvtjI33x8D8RwP7iUEAAAIAmYQUAANAkrAAAAJqEFQAAQJOwAgAAaBJWAAAATcIKAACgSVgBAAA0+R8EA+xAY+93AdgQ1k9sVfZYAQAANAkrAACAJmEFsAPUNn0uYOuzfmK7cI4VwA5hgwLYrKyf2A7ssQIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElYAQAANAkrAACAJmEFAADQJKwAAACahBUAAECTsAIAAGgSVgAAAE3CCgAAoElY8f+3a8cmDMRAFAXX4D7cf1mXuwddA06OZ/tAzMQKNvrwQAAAQCSsAAAAImEFAAAQCSsAAIBIWAEAAETCCgAAIBJWAAAAkbACAACIhBUAAEAkrAAAACJhBQAAEAkrAACASFgBAABEwgoAACASVgAAAJGwAgAAiIQVAABAJKwAAAAiYQUAABAJKwAAgEhYAQAARMIKAAAgElYAAACRsAIAAIiEFQAAQCSsAAAAImEFAAAQCSsAAIBIWAEAAETCCgAAIBJWAAAAkbACAACIhBUAAEAkrAAAACJhBQAAEAkrAACASFgBAABEwgoAACASVgAAAJGwAgAAiIQVAABAJKwAAAAiYQUAABAJKwAAgEhYAQAARMIKAAAgElYAAACRsAIAAIiEFQAAQCSsAAAAImEFAAAQPS++f8/M8YtDgNu87j7gC2wT7GeHbZqxT7Cjj/v0WGv9+xAAAICt+AoIAAAQCSsAAIBIWAEAAETCCgAAIBJWAAAAkbACAACIhBUAAEAkrAAAACJhBQAAEJ2L/JRWF368zgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import metrics_box_example\n",
    "\n",
    "metrics_box_example.evaluate()        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
