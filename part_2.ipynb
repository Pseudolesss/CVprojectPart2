{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Installation\n",
    "## Cytomine:\n",
    "Using pip, you can do it with these two lines:\n",
    "~~~\n",
    "$ curl -s https://packagecloud.io/install/repositories/cytomine-uliege/Cytomine-python-client/script.python.sh | bash\n",
    "$ pip install cytomine-python-client\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cytomine import Cytomine\n",
    "from cytomine.models import ProjectCollection, ImageInstanceCollection, AnnotationCollection\n",
    "import getpass\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.affinity import affine_transform\n",
    "import cv2                             # OpenCV\n",
    "from imgTools import display, multiDisplay\n",
    "import os, sys\n",
    "import logging\n",
    "import segment_detector as sd\n",
    "import numpy as np\n",
    "from metrics_lines import metricNaive, metricFalseNeg, metricFalsePos, metricTot\n",
    "from images_database.preprocess_eyes import img_eye_partial_preprocessing\n",
    "from images_database.preprocess_soccer import preprocessSoccerImage\n",
    "from images_database.augment_data_set_soccers import report_augmented_image_and_annotations\n",
    "from model_ellipses.eyes.predict_image import predict_image as predict_image_eye\n",
    "from model_ellipses.soccers.predict_image import predict_image as predict_image_soccer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image database\n",
    "The evaluation requires the database with all images. If you want to run the evaluation, please provide us the path to the folder(s) containing the images with the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = input(\"Do you want to run the evaluation? (If not, what follows will not be needed) [y, n] : \")\n",
    "if resp == 'n':\n",
    "    EVALUATE = False\n",
    "    PATH_FOLDER_IMG = None\n",
    "else:\n",
    "    EVALUATE = True\n",
    "    print(\"Could you please provide us the path to the folder?\")\n",
    "    PATH_FOLDER_IMG = []\n",
    "    while True:\n",
    "        print(\"\\nThe current list of folders is : \")\n",
    "        for path in PATH_FOLDER_IMG:\n",
    "            print(f'\\t- {path}')\n",
    "        resp = input(\"Do you want to add a new folder? [y, n] : \")\n",
    "        if resp == 'n':\n",
    "            break\n",
    "        else:\n",
    "            resp = input(\"What is the new path? : \")\n",
    "            PATH_FOLDER_IMG.append(resp)\n",
    "\n",
    "if EVALUATE:\n",
    "    print(\"\\n\\nThe final list of folders is : \")\n",
    "    for path in PATH_FOLDER_IMG:\n",
    "        print(f'\\t- {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cytomine connection\n",
    "This is used to get the line annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVALUATE:\n",
    "    host = 'https://learn.cytomine.be'\n",
    "    public_key = getpass.getpass('Public key of cytomine account : ')   # u can get it on your account\n",
    "    private_key = getpass.getpass('Private key of cytomine account : ') # u can get it on your account\n",
    "\n",
    "    CONN = Cytomine.connect(host, public_key, private_key, verbose=logging.ERROR)\n",
    "    # Check https://docs.python.org/3.6/library/logging.html#logging-levels  to see other verbose level\n",
    "    print(CONN.current_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of line detection\n",
    "## Metrics description\n",
    "To evaluate the line detection previously implemented, it was required to choose a metric. The metrics  for segment detection were designed from scratch as no satisfying metrics were found during our research.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**From annoted line parameters to annoted images**\n",
    "\n",
    "The images of the annoted line is obtained by drawing the line with a width of 3 pixels on a black image of the same size. The line width of 3 was chosen arbitrarily to take into consideration the degree of precision of the annotation which is not perfect.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Positive and negative classification**\n",
    "\n",
    "Let's define some important terms : \n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "      \\text{line pixel } &= \\text{ pixel classified as a line by the annotation}\\\\\n",
    "      \\text{not a line pixel } &= \\text{ pixel classified as not a line by the annotation}\\\\\n",
    "      \\text{positive pixel } &= \\text{ pixel classified as a line by the dectection}\\\\\n",
    "      \\text{negative pixel } &= \\text{ pixel classified as not a line by the dectection}\\\\\n",
    "      \\text{true positive pixel } &= \\text{ line pixel classified as a line by the dectection}\\\\\n",
    "      \\text{false positive pixel } &= \\text{ not a line pixel classified as a line by the dectection}\\\\\n",
    "      \\text{true negative pixel } &= \\text{ not a line pixel classified as not a line by the dectection}\\\\\n",
    "      \\text{false negative pixel } &= \\text{ line pixel classified as not a line by the dectection}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Naive metric**\n",
    "\n",
    "The first implemented metric is :\n",
    "$$\\text{metric naive } = \\frac{p_c}{p_t} \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_c = \\text{ number of pixels well classified}\\\\\n",
    "      p_t = \\text{ total number of pixels}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "As the name indicates, it is a very naive metric as the number of line pixels are so low that an image with no lines detected will give a good result.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**False negative metric**\n",
    "\n",
    "The second implemented metric is based on penalization of the false negative pixels :\n",
    "$$\\text{metric false neg. } = min\\big(1, \\frac{1}{a_{neg}}\\frac{p_{tp}}{p_l}\\big) \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_{tp} = \\text{ number of true positive pixels }\\\\\n",
    "      p_{l} = \\text{ total number of line pixels}\\\\\n",
    "      a_{neg} = \\text{ acceptable accuracy for false negative}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "The acceptable accuracy is arbitrarly fixed at $a_{neg} = 0.8$. As the annotation are not perfectly made due to human error, a perfect accuracy should not be expected with this metric either. Thus, using this accepetable metric will simply give the best accuracy score for $\\frac{p_{tp}}{p_l}\\geq 0.8 $.\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "This metrics has the disadvantage of giving a perfect score for a method detecting each pixels as a line. Thus, a new metric is introduced : the false positive metric.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "**False positive metric**\n",
    "\n",
    "This third metric is based on penalization of the false positive pixels:\n",
    "$$\\text{metric false pos. } = min\\big(1, \\frac{1}{a_{pos}}(max(0, 1-\\frac{3\\;p_{fp}}{p_{nl}}))\\big) \\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      p_{fp} = \\text{ number of false positive pixels }\\\\\n",
    "      p_{nl} = \\text{ total number of not a line pixels}\\\\\n",
    "      a_{pos} = \\text{ acceptable accuracy for false positive}\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "As $p_{nl}$ is usually a high number, this metric can easily give a high score, thus the acceptable accuracy for false positive is fixed to $a_{pos} = 1$ and the number of false positive is multiplied by 3 to have a higher penalization.\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1].\n",
    "\n",
    "This metric has the opposite disadvantage of the false negative metric, a method detecting no line on the image will always give a perfect score. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**Total metric**\n",
    "\n",
    "As the two previous metric had opposite disadvantage, it is easy to convince ourself that combining the two will obtain a new metric robust to those two cases.\n",
    "\n",
    "$$\\text{metric tot. } = w_1\\;\\text{metric false neg. } + w_2\\;\\text{metric false pos. }\\quad \\text{with} \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      w_1 \\in [0,1]\\\\\n",
    "      w_2 \\in [0,1]\\\\\n",
    "      w_1 + w_2 = 1\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The metric give a value of the accuracy in the interval [0,1]. The weight were both set to $w_1 = w_2 = 0.5$ such that both the method detecting lines on all pixels and the detecting no lines would have a score of about $0.5$.\n",
    "\n",
    "Note that the false positive metric is very simple and can easily give high score, thus changing it to a better one would give more representative results of the detection method accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example line metrics\n",
    "\n",
    "In this example, the metric for one image is given for 4 cases with the corresponding scores:\n",
    "\n",
    "$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- the actual detection } &: m_{tot} \\simeq 0.7\\\\\n",
    "      \\quad\\text{- all pixels as not a line } &: m_{tot} \\simeq 0.5\\\\\n",
    "      \\quad\\text{- all pixels as line} &: m_{tot} \\simeq 0.5\\\\\n",
    "      \\quad\\text{- all pixels randomly detected as line or not } &: m_{tot} \\simeq 0.3\\\\\n",
    "    \\end{array}\n",
    "\\right.$\n",
    "\n",
    "The score are as expected much better for the actual detection than the other ones. It can be noticed that the false positive metric is quite harsh. It gave a low score to the actual detection and a score of 0 to the randomly pixels. This will cause the total metric to only give good score for image with high number of true positive and very low number of false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_line_example\n",
    "\n",
    "metrics_line_example.evaluate(CONN)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the database\n",
    "Using the metrics described above to evaluate our detection method on the full database will give the corresponding scores:\n",
    "\n",
    "\n",
    "$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric naive } &: m_{nai} = 0.945\\\\\n",
    "      \\quad\\text{- metric false neg. } &: m_{neg} = 0.725\\\\\n",
    "      \\quad\\text{- metric false pos. } &: m_{pos} = 0.860\\\\\n",
    "      \\quad\\text{- metric total } &: m_{tot} = 0.793\\\\\n",
    "    \\end{array}\n",
    "\\right.$\n",
    "\n",
    "The false negative metric has a good accuracy, however, it still has room for improvement. The false positive metric is very high which will improve the total metric accuracy. As explained earlier, the false positive metric is simple and easily give high score, even though it is not that good to the human eye. However it was difficult to find another metric to penalize correctly the false positive pixels.\n",
    "\n",
    "In general, the evaluation indicates that our detection method has good accuracy but could have been better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluateSegDetec\n",
    "\n",
    "if EVALUATE:\n",
    "    resp = input(\"Do you want to evaluate the line detector of part1? \\nThis will takes 2-3 minutes for the full database. [y, n] : \")\n",
    "    if resp == 'y':\n",
    "        evaluateSegDetec.evaluate(PATH_FOLDER_IMG, CONN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of ellipse detection\n",
    "## Metrics description\n",
    "### Distance between two ellipses\n",
    "It is assumed that the ellipses are defined by 5 parameters:\n",
    "- Xc : the vert. coord of the center of the ellipse\n",
    "- Yc : the horiz. coord of the center of the ellipse\n",
    "- theta : the angle of the main axis\n",
    "- a  : half the length of main axis\n",
    "- b  : half the length of sub axis\n",
    "\n",
    "~~~\n",
    "elps1 = [Xc1, Yc1, theta1, a1, b1]\n",
    "elps2 = [Xc2, Yc2, theta2, a2, b2]\n",
    "W = [Wx, Wy, Wt, Wa, Wb]\n",
    "dist = Wx * abs(Xc1-Xc2) + Wy * abs(Yc1-Yc2) + Wt * min(abs(theta1-theta2), abs(180-abs(theta1-theta2))  + Wa * abs(a1-a2) + Wb * abs(b1-b2)\n",
    "~~~\n",
    "The weight W is tuned such that the maximal acceptable error on each parameters will lead to an increase in the distance of 1. Thus if we want an acceptable error of theta of 15 degrees, we fix Wt = 1/15. This way, we can fix the tresh_dist = 5 (see next subsection). This is the maximal acceptable distance between two ellipses. By fixing it at 5, we only need to tune W. The parameters W will depend on the precision expected on the labelled ellipse. \n",
    "\n",
    "Note that as a difference of 180 degree in theta correspond to the same angle, 'min(abs(theta1-theta2), abs(180-abs(theta1-theta2))' is used instead of 'abs(theta1-theta2)'.\n",
    "\n",
    "In practice, the weights used are : $$W = [\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{5}, \\frac{1}{3}, \\frac{1}{3}]$$\n",
    "\n",
    "Example : If we assume that the label for the x coord. of the center is has a precision of two pixels in, i.e there is a two pixels difference between the true x coord and the x coord. labelled, then we can put Wx = 1/2.\n",
    "\n",
    "### Metric on multiple images\n",
    "Let ground_truth and detected both being a list of list of ellipses, respectivelly being the labeled ellipses for each images and the detected ellipses for each images. The evaluation is done on the following pseudo-code:\n",
    "\n",
    "~~~\n",
    "metrics_elps(ground_truth, detected, tresh_dist):\n",
    "    eval = 0\n",
    "    for each image:\n",
    "        gt_elps = ground_truth of the corresponding image\n",
    "        dt_elps = detected of the corresponding image\n",
    "\n",
    "        num = 0\n",
    "        denom = max(len(gt_elps), len(dt_elps))\n",
    "\n",
    "        dists = np.zeros((len(gt_elps), len(dt_elps)))\n",
    "        for el in dists:\n",
    "            el = dist(gt_elps[index1(el)], dt_elps[index2(el)]\n",
    "\n",
    "        index1_rem = []\n",
    "        index2_rem = []\n",
    "        for i in range(min(len(gt_elps), len(dt_elps))):\n",
    "            el = min(dists) where (not index1(el) in index1_rem) and (not index2(el) in index2_rem)\n",
    "            index1_rem.append(index1(el))\n",
    "            index2_rem.append(index2(el))\n",
    "\n",
    "            num += min(1, tresh_dist/el)\n",
    "\n",
    "        eval += num/denom\n",
    "    return eval/num_image\n",
    "~~~\n",
    "\n",
    "In other words, the evaluation metric give a value between 0 and 1, 1 being the best accuracy. The evaluation for multiple images is the mean of all image evaluations. For each image, the evaluation of this images is a fraction. Let $$f(x) = min(1, \\frac{tresh\\_dist}{x}),$$ a function which returns 1 if x < tresh_dist and something < 1 in other cases. The choice of the tresh_dist is done such that a distance lower than it doesn't make sense compared to the precision of the labelling.\n",
    "\n",
    "The numerator is the sum of function f on the smallest distances between the detected ellipses and the labeled ellipses where each detected and labeled ellipse can only be used once. When the number of detected and labeled ellipses aren't the same, the distance are computed until no pair can be made.\n",
    "\n",
    "In the other hand, the denominator is the maximum between the number of detected and labeled ellipses. Thus the value can decrease due to two reasons : if the detection detect less or more ellipses than in the ground truth and if the detected ellipses are distant from the labeled ones.\n",
    "\n",
    "\n",
    "In the special case where there is 0 ellipse in the ground truth the score is 0 if ellipse are detected or 1 if no ellipse are detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of evaluation\n",
    "In the example bellow, the metric for different detection will be displayed. As explained above, the number of ellipse detected will influence the final metric value. To better understand the detection strength and weakness, the metric with or without the penalization for detecting more/less ellipse than existing ellipse will be used.\n",
    "\n",
    "It is tested on 5 cases:\n",
    "    - Same parameters with a 180 change in theta\n",
    "        => metric = 1 for both\n",
    "    - Two ellipses detected with wrong parameters \n",
    "        => it is seen that detecting x times more ellipse than existing ones will divide the score by x (x = 2 here).\n",
    "    - Detecting an ellipse with parameters in the limit of the acceptable error\n",
    "        => metric = 1 for both.\n",
    "    - Detected ellipse close to the ground truth but not exactly it.\n",
    "    - The ground truth detected but other too \n",
    "        => it will give a low score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_elps_example\n",
    "\n",
    "metrics_elps_example.evaluate()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of bounding box detection\n",
    "## Metrics description\n",
    "### Distance between two ebounding box\n",
    "The metric and distance are the same as for the ellipse. The only differences is that there are only 4 parameters (the coordinates of the bottom left corner and top right corner), the weight are $W = [\\frac{1}{2},\\frac{1}{2},\\frac{1}{2},\\frac{1}{2}]$ and the tresh_dist is now equal to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of evaluation\n",
    "In the example bellow, the metric for different detection will be displayed. As explained above, the number of ellipse detected will influence the final metric value. To better understand the detection strength and weakness, the metric with or without the penalization for detecting more/less ellipse than existing ellipse will be used.\n",
    "\n",
    "It is tested on 5 cases:\n",
    "    - Same parameters\n",
    "        => metric = 1 for both\n",
    "    - Two boxes detected with wrong parameters \n",
    "        => it is seen that detecting x times more box than existing ones will divide the score by x (x = 2 here).\n",
    "    - Detecting a box with parameters in the limit of the acceptable error\n",
    "        => metric = 1 for both.\n",
    "    - Detected box far from the ground truth.\n",
    "    - The ground truth detected but other too \n",
    "        => it will give a low score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_box_example\n",
    "\n",
    "metrics_box_example.evaluate()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Conventional methods for ellipses detection\n",
    "\n",
    "The goal here is to find ellipses in an image. The ellipses are then returned as a list of five parameters: the center position, the half lengths of the main axes and the orientation angle of the major axis. On the one hand, we can represent the ellipses and on the other hand count their number if we trust the algorithm enough. We can therefore compare this method to methods based on machine learning.\n",
    "\n",
    "In order to solve this problem, four methods were explored but only two were selected.\n",
    "\n",
    "## Ellipse Hough transform \n",
    "\n",
    "This method has been tried but not selected for further works due to slowness.\n",
    "\n",
    "### Description\n",
    "This method works like the usual hough transform. Depending on its implementation, it will take a certain number of points and accumulate the parameters of the ellipses that could pass through these points. Once all points have been tested, the accumulators are evaluated and all parameters exceeding a certain threshold are selected as good candidates to represent an ellipse. These parameters will then form the different ellipses detected by the algorithm.\n",
    "\n",
    "### Pros\n",
    "* Very robust against noise\n",
    "* Very robust against partial occlusion\n",
    "* Easy to implement / understand\n",
    "\n",
    "### Cons\n",
    "* Takes \"age of the univers\" to execute, not tractable for huge images\n",
    "* Not suitable for embedded computing / live applications\n",
    "\n",
    "## Hough transform with parameter separation\n",
    "\n",
    "This method has been implemented up to center detection but not selected for further works due to high bias and low performance on images containing several ellipses.\n",
    "\n",
    "### Description\n",
    "This method is very similar to the previous one. Here what will be tried is to separate the imposing transform into several small transforms. The one we studied is the one presented by Le Troter et al. (Arc of ellipse detection for video image registration, 2005) . The idea is to first look for the center of the ellipses and then find the other parameters from the center. To do this, triplets of points are selected. For each of them, the most likely tangent is evaluated by filtering (a mask is created for each tangent and the one that most closely resembles the neighborhood of the point is chosen). \n",
    "\n",
    "Then, a property of the ellipses allows to find the center of the ellipse formed by these three points. In fact, we know that for two points, if we link the intersection of their tangent to their center, the line thus formed will pass through the center of the ellipse. The crossing of the two lines thus formed by the triplet is therefore the hypothetical centre of an ellipse. These centers are accumulated and the ones with the highest scores are chosen as actual ellipse centers.\n",
    "Arc of ellipse detection for video image registration\n",
    "From there, the three remaining parameters are accumulated.\n",
    "\n",
    "### Pros\n",
    "* Faster than Ellipse Hough transform\n",
    "* Possibly suitable for embedded computing and live applications\n",
    "\n",
    "### Cons\n",
    "* Very sensitive to accuracy errors\n",
    "\n",
    "## Ellipse detection by accumulation of the secondary axis\n",
    "\n",
    "This method is fully implemented and used.\n",
    "\n",
    "### Description\n",
    "\n",
    "This technique follows what was proposed by Yonghong Xie and Qiang Ji (A New Efficient Ellipse Detection Method, 2002). The idea will be to consider all pairs of points as the main axis of an ellipse. Then, for all the other points, we'll pretend they're on the ellipse. From these three points (the new selected point and the ends of the major axis), the half length of the secondary axis is evaluated and accumulated. Major axes with a secondary axis whose score exceeds a certain threshold are retained as the parameters of an ellipse. As it is also possible to find the other parameters from what is known about the ellipse, the ellipse is totally known at this point. Indeed, the center of the ellipse corresponds to the center of the two points chosen as the ends of the major axis. The angle of the major axis can also be evaluated from these two points.\n",
    "\n",
    "Two sub-implementations have been used here. The first is to remove the pixels from the image once an ellipse has been detected (this avoids duplicates). The other implementation keeps all ellipses but only returns the one that received the most votes. The second method has the advantage of being more accurate but it is also slower and does not detect more than one ellipse.\n",
    "\n",
    "### Pros\n",
    "* Way faster than Ellipse Hough transform\n",
    "* Suitable for embedded computing and live applications\n",
    "\n",
    "### Cons\n",
    "* Sensitive to noise (as an example, this method doesn't work well on soccer images due to the lines. A solution would be to first remove the lines here).\n",
    "\n",
    "### Demo\n",
    "\n",
    "The demonstration uses our implementation of the Yonghong Xie and Qiang Ji algorithm with pixel removal. The algorithm has 6 parameters which are the minimum and maximum half lengths of the two main axes, the number of bags in the accumulator and the threshold. The images used are only from the eye dataset because it takes a long time to find the ellipses in soccer images as their 'white pixel / black pixel' ratio is higher. Also, the algorithm was implemented in python and not C++ or full numpy which tends to slow down the computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import classical as cl\n",
    "\n",
    "original_eye = cv2.imread(\"./ReportImages/elps_eye01_2014-11-26_08-54-45-007.png\", cv2.IMREAD_GRAYSCALE)\n",
    "preprocess_eye = img_eye_partial_preprocessing(original_eye)\n",
    "\n",
    "imageBinary = cv2.Canny(preprocess_eye, 20, 100)\n",
    "\n",
    "ellipses = cl.youghongQiangEllipse(imageBinary, 15, 55, 5, 25, 40, 10, onlyBest=False, ratio=0.5)\n",
    "\n",
    "imageBGR = cv2.cvtColor(original_eye, cv2.COLOR_GRAY2BGR)\n",
    "for ell in ellipses:\n",
    "    cv2.ellipse(imageBGR, ell[0], ell[1], ell[2], 0, 360, (0, 0, 255))\n",
    "    \n",
    "multiDisplay([\"Pre-filtered image\", \"Result\"], [imageBinary, imageBGR], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the algorithm in this configuration detects 0 or more ellipses, not always a single one. This is why the \"onlyBest\" option is there to output only the best ellipse and not all those exceeding a certain threshold. However, the execution of this version of the algorithm is much slower. The reader can try it by modifying the \"onlyBest\" argument of the yongQiangEllipse function but the test is not performed by default in the notebook.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the ellipse detection by accumulation of the secondary axis on the database\n",
    "Using the metrics described above to evaluate our detection method on the full database will give the corresponding scores:\n",
    "\n",
    "- On the eye images with 1 ellipse\n",
    "$$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric penalizing } &: m_{p} = 0.36\\\\\n",
    "      \\quad\\text{- metric non penalizing } &: m_{np} = 0.58\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "- On the eye images with 0 ellipse\n",
    "$$\n",
    "\\left.\n",
    "    \\begin{array}{ll}\n",
    "      \\quad\\text{- metric penalizing } &: m_{p} = 0.21\\\\\n",
    "      \\quad\\text{- metric non penalizing } &: m_{np} = 0.21\\\\\n",
    "    \\end{array}\n",
    "\\right.$$\n",
    "\n",
    "In the case of the images with 1 ellipse, the average number of ellipses detected is : $\\frac{0.58}{0.36} = 1.61$ which is pretty good. However, in the case of 0 ellipse on the image, only 1 image out of 5 did the algorithm detect no ellipse which isn't that good. Going back to the case of 1 ellipse, the metric non penalizing reach a good accuracy.\n",
    "\n",
    "Giving our weights, assuming that the error in split equally on the parameters, the ellipse detected will have a distance to the ground truth ellipse of : \n",
    "$$\\frac{2}{0.6} = 3.33\\text{ pixels of Xc}, 3.33\\text{ pixels of Yc}, 8.33\\text{ degrees of theta},5 \\text{ pixels of a},5 \\text{ pixels of b}  $$\n",
    "\n",
    "In conclusion, the classical method has a good accuracy but struggle to detect the exact number of ellipses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluateElpsClassical\n",
    "\n",
    "if EVALUATE:\n",
    "    resp = input(\"Do you want to evaluate the line detector of part1? \\nThis will takes several minutes for the full database. [y, n] : \")\n",
    "    if resp == 'y':\n",
    "        evaluateElpsClassical.main(PATH_FOLDER_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "The processing for the eyes and the soccer image dataset applied is different because of the different objective aims for each of them.\n",
    "\n",
    "## Eyes\n",
    "\n",
    "The aim for this dataset is to isolate the iris of the eye. This is done through multiple steps. The particularity of the iris is that it contains the darkest pixel of the images. So the main procedure will rely on increasing the contrast of the image. \n",
    "\n",
    "First, we proceed to a normalisation of the gray-scale image. Then we increase the contrast by multiplying all the grayscale values by a given factor. The factor has been chosen according to the value histogram of the whole dataset with some manual tuning. We had chosen to use a 15 factor because it was a good trade-off for isolating the iris and reduce the negative impact of some group of images with eye-liners which are of the same level of grey or darker than the iris.\n",
    "\n",
    "Then, we apply a binary threshold by applying the Otsu algorithm to define the threshold. It succeeds globally in splitting the value histogram between the skin/rest of the eye and the iris (potentially eyelashes). These two successive operations are similar to an increase in contrast.  Finally, we finish the preprocessing with dilatation to close the shape of the iris. A close operation wasn't needed because they weren't any isolated small artifact after the increase in contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_eye = cv2.imread(\"./ReportImages/elps_eye01_2014-11-26_08-54-45-007.png\", cv2.IMREAD_GRAYSCALE)\n",
    "preprocess_eye = img_eye_partial_preprocessing(original_eye)\n",
    "multiDisplay([\"Preprocess Eye\", \"Original Eye\"], [original_eye, preprocess_eye], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soccer\n",
    "\n",
    "The aim for this dataset is to isolate the line of the soccer field the best as we can. This is rather more complex than the preceding context. The end of this process was inspired from a paper from the Sharif University of Technology in Iran. \"Automatic Soccer Field Line Recognition by Minimum Information\" : http://confnews.um.ac.ir/images/41/conferences/aisp2015/183_3.pdf. \n",
    "\n",
    "The following step where apply to the image after blurring to reduce the complexity of some information and uniform the images in its different colour space.\n",
    "\n",
    "We first take back our HSV mask approch from the first part of this project. By applying an HSV mask, we are able to catch all the pixel corresponding to the green spectrum by determining the range of the Hue value. The range was defined by the histogram and manual adjustment. This first step isolates most of the soccer field.\n",
    "\n",
    "Then, we keep only the biggest non black 4-way connected component of the images in order to remove all artefact outside the soccer field. Before doing this, we apply a dilatation operation with a horizontal rectangle kernel. Because, when processing a image from the side of the field, the central line of the field is composed of white value corresponding to the whole Hue range and some dirt (brown Hue range) could be observed on its border in a considerable number of pixels. This issue splits the soccer field into two 4-way connected components. But the horizontal rectangle dilatation succeeds in connected these two components without connecting them to outsider artefacts. \n",
    "\n",
    "When this is done, we add the remaining White value missing from the first mask by applying a range on the saturation and the value, low saturation range [0, 30] and high value [190, 255]. These values were defined by observation on some case because the histogram wasn't helping significantly to identify a pic. After applying the 2nd HSV mask, we proceed again in an isolation of the biggest 4-way connected component.  \n",
    "\n",
    "The last part is focused on retrieving the lines. The quote paper explains the approach we apply in a small section. The main idea is to proceed to a median filtering in the LAB colour space on the L channel only. The L channel represents luminescence value and is not equivalent to the Grey Scale nor the Value channel of HSV because it applies a nonlinear transformation of the RGB space. This colour space is slightly better for filtering operation because it better dissociates brightness and chromatic data. The median filtering will help to uniform the luminescence over the soccer field and its dimension were chosen to be able to drop the luminescence data from the white line. Then we subtract the image before the operation and the filtered image. After applying a binary threshold (\\[0, 10\\]\\[11, 255\\]) and remove small connected component (closing operation could have wipe out thin white lines), we have our final prepossessing image which consists of all bright pixel on the soccer field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original_soccer = cv2.imread(\"./ReportImages/elps_soccer01_1024.png\", cv2.IMREAD_COLOR)\n",
    "preprocess_soccer = preprocessSoccerImage(original_soccer)\n",
    "multiDisplay([\"Preprocess Soccer\", \"Original Soccer\"], [original_soccer, preprocess_soccer], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The objective was to extend the dataset of soccer images because it is by far the most complicated case for the deep learning model. In order to get synthetic images with synthetic annotations, we based our approach on the already preprocessed dataset from the original soccer images dataset. Because the preprocessing parameters are really sensible to a certain level of zoom and we aren't sure of how the preprocess of modified images will result. It is also a good way to save resources and to get as fast as possible new data. The lines are already extracted so we applied changes to these images to simulate plausible contexts. We are using the following affine transformations:\n",
    "\n",
    "- Horizontal Flip\n",
    "- Zoom [0.8, 0.95] (Zoom in) & [1.2, 2] (Zoom out)\n",
    "- Shift according to y axis  +/- [0.05 * height, 0.2 * height]\n",
    "\n",
    "The parameter used are chosen from the previous range according to a uniform law to not bias the simulation and to get something different from the original prepossessed image. Because shifting according to x and zoom in are excluding ellipses from the bounds, we didn't apply or restraint the factor. We use this new augmented dataset according to the variation in accuracy in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "soccer_image = cv2.imread(\"./ReportImages/elps_soccer01_1056.png\" , cv2.IMREAD_GRAYSCALE)\n",
    "flip_image = report_augmented_image_and_annotations(\"FLIP\", soccer_image, \"elps_soccer01_1056.png\", \"ReportImages/CV2019_Annots_ElpsSoccer.csv\")\n",
    "zoom_image = report_augmented_image_and_annotations(\"ZOOM\", soccer_image, \"elps_soccer01_1056.png\", \"ReportImages/CV2019_Annots_ElpsSoccer.csv\")\n",
    "shift_image = report_augmented_image_and_annotations(\"SHIFT\", soccer_image, \"elps_soccer01_1056.png\", \"ReportImages/CV2019_Annots_ElpsSoccer.csv\")\n",
    "\n",
    "multiDisplay([\"Original Image\", \"Horizontal Flipped Image\", \"Zoom In Image\", \"Shift Up Image\"], [soccer_image, flip_image, zoom_image, shift_image], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "One method we use to tackle the main goal of the project, the detection of ellipses,\n",
    "is the deep learning. \n",
    "We use keras and tensorflow to build, train and evaluate deep learning models. One for each of the \n",
    "asked tasks : classification of eye images ( 0 or 1 ellipse ), classification of soccer images ( 0, 1, 2, 3 ellipses ), \n",
    "regression on eye images ( 5 parameters of the ellipse ), regression on soccer image ( 4 parameters of the bounding box \n",
    "of the ellipse ). \n",
    "To do do all that we need to extract and prepare the appropriate data, define the models, \n",
    "train them and evaluate there results. After doing all that we can predict any images.\n",
    "Soccer and Eye have the same file names in different folders (*model_ellipses/eyes* and *model_ellipses/soccers*)\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "The data is extracted in *get_model_data* functions.\n",
    "We extract the annotations from the .csv file (for annotations of original images) and .pkl file (for \n",
    "annotations of generated images created in *augment_data_set_soccers*). \n",
    "In eye and soccer we make two lists, one with the preprocessed images (in np.array format) (which will be the input of our regressions)\n",
    "and one of the same length \n",
    "with the corresponding annotations (bouding box parameters or ellipse parameters) (which will be used as correct output in regression)\n",
    ". An important note is that the regression in soccer is done for one ellipse at a time.\n",
    "The concept we find is that we take in the annotation list only the biggest annotation.\n",
    "The model is so trained to obtain only the biggest ellipse of an image\n",
    " In soccer we make also an array of the number of ellipse for each image.\n",
    "For the classification we also use the folder of images without ellipses.\n",
    "The classification of soccer take as input the image and as output the number of ellipse\n",
    "(which is obviously 0 if the image comes from the without ellipse directory)\n",
    "In the case of eye, the output is True if from the ellipses directory and False if from the without ellipse directory \n",
    "\n",
    "## Model definition\n",
    "\n",
    "A choice has been made to create and train our models by ourselves. \n",
    "We start from scratch, without using pre-trained models. There are all \"simple\" convolutionnal models.\n",
    "\n",
    "The four models had a lot in common. There are defined in *models.py*\n",
    "They are composed of multiple layers. With at the beginning convolutionnal and pooling layers.\n",
    "The input image is of size 320x180 for soccer (the input images are downscaled from 720p and 1080p\n",
    "definition during the data preparation) and 320x180 for eye (original size)\n",
    "We have a flatten layer and after that a certain number of fully-connected (dense) layers.\n",
    "At the end the output go through a activation function corresponding of the type of output wanted\n",
    "( classification vs regression ). Between some layers we put the very important dropouts.\n",
    "Dropout indicate that each time, during training, a percentage of nodes are \"desactivated\", the weights\n",
    "are set to 0. It is useful to force the neural network to \"understand\" better the data by discovering new ways to \n",
    "decrease the loss. It reduces a lot overfitting over time. The add of these layers had greatly improve the results.\n",
    "Without them, it was impossible to do a lot of epochs, the networks was too overfitting.\n",
    "\n",
    "## Model training and evaluation\n",
    "\n",
    "The training process is relatively simple. We take the prepared data, we do some other preparations\n",
    "(for example in the soccer classification the output values must be translated from int to array of boolean\n",
    "like 2 -> \\[0 0 1 0]) and separate the data between testing and training/validation data.\n",
    "In the eye classification we use an equivalent method which consits in having the data not from the \n",
    "input of the function but from directory that the keras generators will directly use.\n",
    "The two classifications make the fit of the model using the generators.\n",
    "The advantage of the generators is the gestion of the RAM and the possibility to slighty randomly modify the data\n",
    "to have various results. The resulting loss is slighty improved in eye classification and significantly \n",
    "improved in soccer classification using this method instead of the normal fit that we use \n",
    "for regression. \n",
    "The model is evaluate using the testing data. The results in term of loss an accuracy are described below.\n",
    "After that we can save the model in files, to be usable when we want to predict an image.\n",
    "After experimenting different optimizers, we choose to use for all the models the Adadelta optimizers which gives \n",
    "the better results  \n",
    "\n",
    "### eye classification\n",
    "\n",
    "Simple model which doesn't need a lot of dense layers to obtain good results. It is normal because \n",
    "a binary classification of well preprocessed image (the ellipses are clear in the image)\n",
    "is a simple tasks.\n",
    "The result is 97% of accuracy, which means that 97% of eye images are correctly classified.\n",
    "\n",
    "### soccer classification\n",
    "\n",
    "The model is a little more complex. The classification is done with 4 outputs which make it more difficult.\n",
    "We put one dense layer before the final layer  and some dropouts. After some experimenentations\n",
    "concerning the best number of epochs, values of dropout, values of dense, we obtain a 95%\n",
    "accuracy model, which is very good considering that some data are difficult to take account\n",
    "correctly, like 3 ellipses images with one very small for example. \n",
    "\n",
    "### eye regression\n",
    "\n",
    "The model is more complex. The goal of the regression is more complex than classification\n",
    "and need not only \"space\" reflexions given by convolutional networks but also a lot of fully connected layers.\n",
    "This model uses a custom loss because not all the parameters had the same importance . The majority of eyes\n",
    "are relatively round, so a difference of angle is less visible than a difference of center position,\n",
    "which is very important because the ellipses are relatively little in the image and the more \n",
    "important is to minimize the distance between real center and computed center.\n",
    "The dropouts are also very useful.\n",
    "This model use in option some generated data which gives a bigger database. The results are a little better\n",
    "using the extended data but it takes a lot more time to extract data and it multiply the computation\n",
    "time. \n",
    "\n",
    "After a lot of experimentation about the number of dense, the dropout value, the custom loss\n",
    "weight, we find a final regression model with a total loss of 311 and accuracy of 94.\n",
    "\n",
    "### soccer regression\n",
    "\n",
    "The model is the more complex we had developed. The task is more complex than the others. Here we doesn't want\n",
    "the parameters of one ellipse, we want only the biggest ellipse. \n",
    "To do that, it is necessary to have multiple fully connected layers in order that the model can\n",
    "understand well what we ask to it. \n",
    "After a lot of time it was not working. We finally obtain a correct results by increasing \n",
    "significantly the number of dense layers and the epochs.\n",
    "The computations takes a lot of time compared to the others but the model ends by converging.\n",
    "We obtain finally a loss of 243 (during a long time the results was above 2000)\n",
    "\n",
    "## Predict image functions\n",
    "\n",
    "Finally we have the function which make all the work of predicting the ellipses.\n",
    "It takes a path to an image. Do preprocessing. Load the best classification model.\n",
    "Use the model to predict the number of ellipses (which is sent a the end).\n",
    "After that, if we are in eye and the classification tell that there is one ellipse,\n",
    "We load the best regression model, precict the paramters of the ellipse and sent them as \n",
    "output. We plot also the results to see visually what happen.\n",
    "If we are in soccer, we make a loop a number of time equal to the number of detected ellipse\n",
    "Each time we predict the parameters of the box and save it, remove the box of the image,\n",
    "and predict on the masked image in order to find the biggest ellipse of the remaining.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The results obtains are globally very satisfying. The classification works for a high percentage of \n",
    "images and the regression gives in general results which makes sense despite the difficulties \n",
    "inherent to the problem (also the fact that the manual annotations are not really precise)\n",
    "and that we use our personnal simple models, not already trained complex networks.\n",
    "\n",
    "To observe the results, you can use *model/ellipses/eyes/predict_image.py* and\n",
    "*model/ellipses/soccers/predict_image.py*. The functions take simply the path of your input image,\n",
    "print the resulting images and give the ellipses as asked in the specification\n",
    "The computation takes a certain time. \n",
    "\n",
    "If you want to train the model, you can just run the *model/ellipses/... /train_... * files \n",
    "The computation takes hours if not with a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Put True if you want to test (could take time)\n",
    "if False:\n",
    "    source = \".\"\n",
    "    predict_image_eye(source+\"images_database/Team03/elps_eye10_2015-01-29_09-01-45-005.png\")\n",
    "    predict_image_soccer(source+\"images_database/Team03/elps_soccer01_1411.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained model To obtain bounding boxes\n",
    "\n",
    "RetinaNet is a state of the art object detector made up of multiple layers of neural networks. It uses a ResNet as a backbone deep feature extractor. Over the ResNet, a Feature Pyramid Network is used to detect objects at different scales. Finally, a classification subnet and box regression subnet are used to classify and put the bounding boxes on the images. \n",
    "It performs very well on small objects, animal and human detection. Its accuracy is similar to the one of ResNet-101-FPN Faster R-CNN while being faster and it also outperforms single-shot detector.\n",
    "The git associated : https://github.com/fizyr/keras-retinanet\n",
    "\n",
    "More information and the code are available under retinaNotebook.ipynb which shall be run on google colab for simplicity purposes. \n",
    "\n",
    "The RetinaNet was trained over the soccer images starting with the pretrained weights: https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5 with a batch size of 20, 50 steps over 100 epochs which correspond to more than 8 hours of training.\n",
    "\n",
    "These results are not good enough.First, when we look at the models predicted accuracy, the outputed accuracies are most of the time lower than 50%. This leads to a lot of tuning to choose the right threshold for the outputed accuracy. Furthermore,the false positive rate is too high and the boxes even when on the right area are not precise enough. \n",
    "\n",
    "By using a pretrained network, we only train part of the network, usually only the last layers are trained but in this case the fact that the inside of ellipses on the soccer field are of the same texture and color as the outside seems to make it very difficult for the network to find a suitable solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imgTools import display, multiDisplay\n",
    "\n",
    "retinaSoccer = cv2.imread(\"./ReportImages/retina2.png\",cv2.IMREAD_COLOR )\n",
    "retinaSoccerFP = cv2.imread(\"./ReportImages/retina5.png\",cv2.IMREAD_COLOR )\n",
    "retinaSoccerT30 = cv2.imread(\"./ReportImages/retina3.png\",cv2.IMREAD_COLOR )\n",
    "retinaSoccerT45 = cv2.imread(\"./ReportImages/retina4.png\",cv2.IMREAD_COLOR )\n",
    "\n",
    "multiDisplay([\"Central ellipse\", \"False positive\",\"Threshold = 0.3\", \"Threshold = 0.45\"], [retinaSoccer, retinaSoccerFP, retinaSoccerT30, retinaSoccerT45],2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
